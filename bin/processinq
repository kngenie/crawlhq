#!/usr/bin/python
#
import sys, os
from urllib import urlencode
from urllib2 import urlopen, URLError, HTTPError
import httplib
from optparse import OptionParser
import json
import time

opt = OptionParser(usage='%prog [OPTIONS]')

opt.add_option('-D', '--detatch', action='store_true', dest='detatch',
               default=False, help='run in background, detatched')
opt.add_option('-L', '--logfile', action='store', dest='logfile',
               default=None, help='a file to send log output to (default %default)')
opt.add_option('-q', '--hq', action='store', dest='hqhost', default='localhost',
               help='HQ hostname (default "%default")')
opt.add_option('-j', '--job', action='store', dest='job', default='wide',
               help='job name (default "%default")')
opt.add_option('-n', action='store', dest='max', type='int', default=500,
               help='maximum number of URLs to process in one call')
opt.add_option('-e', '--empty-wait', action='store', dest='empty_wait',
               type='int', default=30,
               help='seconds to wait before making next call when queue gets exhausted')
opt.add_option('-E', '--error-wait', action='store', dest='error_wait',
               type='int', default=120,
               help='seconds to wait before making next call when processinq'
               ' call failed (%default)')
opt.add_option('-1', action='store_true', dest='oneshot',
               default=False, help='run processinq just once')
opt.add_option('--max-errors', action='store', dest='max_errors', type='int',
               default=5,
               help='processinq will exit after this number of consecutive'
               ' 500/503 server errors. default %default, set to 0 to disable exit')

options, args = opt.parse_args()

if options.detatch:
    sys.stdout = sys.stderr = open(options.logfile or '/dev/null', 'a', 0)
    pid = os.fork()
    if pid > 0:
        print >>sys.stderr, "pid=%d" % pid
        sys.exit(0)
    os.setsid()
else:
    if options.logfile:
        sys.stdout = sys.stderr = open(options.logfile, 'a', 0)

URL_PROCESSINQ = 'http://%s/hq/jobs/%s/processinq?%s' % (
    options.hqhost, options.job,
    urlencode(dict(max=options.max)))

empty_wait = options.empty_wait
error_wait = options.error_wait

consecutive_errors = 0
while 1:
    try:
        f = urlopen(URL_PROCESSINQ)
        c = f.read().rstrip()
        f.close()
        r = json.loads(c)
        try:
            r['ps'] = r['processed']/r['t']
        except:
            r['ps'] = 0.0
        print >>sys.stderr, "%(job)s %(scheduled)d/%(processed)d X:%(excluded)d T:%(t).3f(D%(td).3f,S%(ts).3f) %(ps).2f/s" % r
        if 'error' in r:
            print >>sys.stderr, "  error: %s" % r['error']
            print >>sys.stderr, "  sleeping %s" % error_wait
            time.sleep(float(error_wait))
            # TODO: should apply max_error, here?
            continue
        processed = r.get('processed')
        if options.oneshot: break
        if processed > 0:
            pass
        else:
            print >>sys.stderr, "  inq exhausted, sleeping %ds" % empty_wait
            time.sleep(float(empty_wait))
        consecutive_errors = 0
    except httplib.HTTPException as ex:
        # typically httplib.BadStatusLine
        print >>sys.stderr, ex
        consecutive_errors += 1
        print >>sys.stderr, "  sleeping %s" % error_wait
        time.sleep(float(error_wait))
    except HTTPError as ex:
        print >>sys.stderr, ex
        if ex.getcode() in (500, 503):
            consecutive_errors += 1
            print >>sys.stderr, "error %d" % consecutive_errors
            if consecutive_errors >= options.max_errors > 0:
                print >>sys.stderr, "exiting"
                break
        print >>sys.stderr, "  sleeping %ds" % error_wait
        time.sleep(float(error_wait))
    except KeyboardInterrupt:
        break
