#!/usr/bin/python
# stand-alone (non-webapp) version of seenchecker-scheduler.
#
import sys, os
sys.path[0:0] = (os.path.join(os.path.dirname(__file__), '../lib'),)
import re
from optparse import OptionParser
from threading import Thread, Condition, RLock
import time
import traceback
import logging
import signal
import pymongo
try:
    import tornado.ioloop
    import tornado.web
except:
    tornado = None

import hqconfig
from mongojobconfigs import JobConfigs
from mongodomaininfo import DomainInfo
from dispatcher import Dispatcher, WorksetMapper
from scheduler import Scheduler

DEFAULT_BINDADDR = ('127.0.0.1', 8819)

# if tornado:
#     class API(tornado.web.RequestHandler):
        
def interrupted(sig, frame):
    global repeat
    print >>sys.stderr, "Interrupted, exiting..."
    repeat = False

# temporary - this is a copy of CrawlMapper in hq.py (move it to lib)
class CrawlMapper(WorksetMapper):
    """maps client queue id to set of WorkSets
    """
    def __init__(self, crawljob, nworksets_bits):
        super(CrawlMapper, self).__init__(nworksets_bits)
        self.crawljob = crawljob
        self.jobconfigs = self.crawljob.jobconfigs
        self.job = self.crawljob.jobname
        #self.nworksets = (1 << nworksets_bits)
        self.load_workset_assignment()

        self.client_last_active = {}

    def create_default_workset_assignment(self):
        num_nodes = self.jobconfigs.get_jobconf(self.job, 'nodes', 20)
        return list(itertools.islice(
                itertools.cycle(xrange(num_nodes)),
                0, self.nworksets))
        
    def load_workset_assignment(self):
        wscl = self.jobconfigs.get_jobconf(self.job, 'wscl')
        if wscl is None:
            wscl = self.create_default_workset_assignment()
            self.jobconfigs.save_jobconf(self.job, 'wscl', wscl)
        if len(wscl) > self.nworksets:
            wscl[self.nworksets:] = ()
        elif len(wscl) < self.nworksets:
            wscl.extend(itertools.repeat(None, self.nworksets-len(wscl)))
        self.worksetclient = wscl

    def wsidforclient(self, clid):
        '''return list of workset ids for node name of nodes-node cluster'''
        # XXX linear search
        qids = [i for i in xrange(len(self.worksetclient))
                if self.worksetclient[i] == clid]
        return qids

    def client_activating(self, clid):
        """called back by Scheduler when client gets a new feed request
        when it is inactive."""
        for wsid in self.wsidforclient(clid):
            self.crawljob.workset_activating(wsid)

class CrawlJob(object):
    """just enough to satisfy CrawlMapper requirements"""
    def __init__(self, name, jobconfigs):
        self.jobname = name
        self.jobconfigs = jobconfigs

opt = OptionParser(usage='%prog [OPTIONS] JOBNAME')
opt.add_option('-n', action='store', dest='maxn', default=500, type='int',
               help='maximum number of URLs to process (advisory)')
opt.add_option('-1', action='store_true', dest='justonce', default=False,
               help="runs processinq just once, then exits"
               )
opt.add_option('-w', action='store', dest='exhaust_delay', default=60,
               type='int',
               help='seconds to wait between processing when queue '
               'got exhausted')
opt.add_option('-L', action='store', dest='listen',
               default=('%s:%d' % DEFAULT_BINDADDR),
               help='address:port to bind HTTP API end-point to. '
               'specify "none" to disable HTTP API')
opt.add_option('-v', action='store_true', dest='verbose', default=False,
               help='print out extra information')
options, args = opt.parse_args()

logging.basicConfig(level=(logging.DEBUG if options.verbose else logging.INFO),
                    format='%(asctime)s %(levelname)s %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')

listen = options.listen
if listen == 'none':
    hostport = None
else:
    hostport = listen.split(':', 1)
    if len(hostport) == 1: hostport.append(DEFAULT_BINDADDR[1])
    if hostport[0] == '': hostport[0] = DEFAULT_BINDADDR[0]
    hostport[1] = int(hostport[1])

if len(args) < 1:
    opt.error('JOBNAME is missing')
job, = args

repeat = not options.justonce
do_flush = False

mongo = pymongo.Connection(hqconfig.get('mongo'))
domaininfo = DomainInfo(mongo.crawl)
jobconfigs = JobConfigs(mongo.crawl)
mapper = CrawlMapper(CrawlJob(job, jobconfigs), hqconfig.NWORKSETS_BITS)
scheduler = Scheduler(hqconfig.worksetdir(job), mapper, reading=False)
dispatcher = Dispatcher(domaininfo, job, mapper=mapper, scheduler=scheduler)

if hostport and tornado:
    # TODO
    class APIHandler(tornado.web.RequestHandler):
        def initialize(self, dispatcher):
            self.dispatcher = dispatcher
        def get(self, a):
            h = getattr(self, 'get_'+a, None)
            if not h:
                self.send_error(404, msg=('undefined action %s' % a))
                return
            try:
                h()
            except Exception as ex:
                self.write(dict(success=0, error=str(ex)))
            self.finish()
        def write_error(self, msg=''):
            self.write(dict(success=0, error=msg))

        def get_flush(self):
            job = self.get_argument('job')
            self.write(dict(success=1, r=self.dispatcher.flush_job(job)))
        def get_clearseen(self):
            job = self.get_argument('job')
            self.write(dict(success=1,
                            r=self.dispatcher.get_job(job).seen.clear()))
        def get_status(self):
            job = self.get_argument('job')
            self.write(dict(success=1,
                            r=self.dispatcher.get_job(job).get_status()))

    application = tornado.web.Application([
            (r'/(.*)', APIHandler, {'dispatcher':dispatcher}),
            ])
    application.listen(hostport[1], address=hostport[0])
    appth = Thread(target=tornado.ioloop.IOLoop.instance().start)
    appth.setDaemon(True)
    appth.start()

signal.signal(signal.SIGINT, interrupted)
try:
    while 1:
        # t and job should be set in Dispatcher.processinq
        start = time.time()
        r = dispatcher.processinq(options.maxn)
        r['t'] = time.time() - start
        r['job'] = job
        r.update(
            ps=r.get('processed', 0)/r['t'],
            eps=r.get('scheduled', 0)/r['t']
            )
        print >>sys.stderr, "%(job)s %(scheduled)d/%(processed)d X:%(excluded)d T:%(t).3f(D%(td).3f,S%(ts).3f) %(eps)8.2f/%(ps)8.2f/s" % r
        if not r.get('processed'):
            # flush worksets before sleeping - this is not perfect,
            # but better-than-nothing solution until we implement
            # inter-process communication.
            if do_flush:
                dispatcher.flush()
                do_flush = False
            logging.info("inq exhausted, sleeping for %ds",
                         options.exhaust_delay)
            # because wait_available cannot be interrupted by INT signal,
            # we repeat short waits for exhaust_delay.
            #time.sleep(options.exhaust_delay)
            until = time.time() + options.exhaust_delay
            while time.time() < until and repeat:
                if dispatcher.wait_available(1.0):
                    break
        else:
            do_flush = (r.get('scheduled', 0) > 0)
        if not repeat:
            break
finally:
    dispatcher.shutdown()
    jobconfigs.shutdown()
    domaininfo.shutdown()
    mongo.disconnect()

